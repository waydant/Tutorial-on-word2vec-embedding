{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6EviZmZXnkS"
      },
      "source": [
        "# Analyze word2vec embedding by demonstrating clustering on a subtitle file of a movie of your choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qfeLe8cXnkT"
      },
      "source": [
        "#### Vedant Gaurang Shah; 200101115"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi4Jmay6XnkU"
      },
      "source": [
        "### Understanding Word2Vec\n",
        "Word2Vec is a powerful technique for generating word embeddings, which are numerical representations of words that capture their semantic meaning. These embeddings allow us to understand the relationships among words; for example, similar words tend to have similar vector representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwK1Dd-4XnkU"
      },
      "source": [
        "![Overview of Word2vec](./overview_word2vec.drawio.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy8KgMC1XnkV"
      },
      "source": [
        "The vectors used to represent the words are called neural word embeddings, and representations are strange. One thing describes another, even though those two things are radically different. Word2vec \"vectorizes\" about words, and by doing so it makes natural language computer-readable - we can start to perform powerful mathematical operations on words to detect their similarities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGBtiQbwXnkV"
      },
      "source": [
        "Word2Vec is one of the most popular technique for **word embeddings** (??, what are those?) using shallow neural networks.\n",
        "\n",
        "**Word Embedding** is one of the most popular representation of document vobulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc (as told before).\n",
        "\n",
        "So loosely speaking, **word embeddings** are vector representations of a particular word.\n",
        "Now the question, how are textual words converted to some mathematical, numerical vectors? How do we generate them?? And how do they capture the context??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3tROLfDXnkV"
      },
      "source": [
        "There are 2 methods for word2vec, such as **CBOW** and **Skip-Gram**. Given a set of sentences (also called corpus), the model loops on the words of each sentences and either try to use the current word w in order to predict its neighbors (i.e, its context), this approach is called \"Skip-Gram\", or its employs each of these contexts to predict the current word _w_, in that case, the approach is called \"Continuous Bag of Words\" (CBOW). To limit the number of words in each context and tune the performance of the model, a parameter called \"window size\" is used.\n",
        "\n",
        "Simply stating:\n",
        "* CBOW (Continuous Bag of Words): Predicts a target word based on the surrounding context.\n",
        "* Skip-gram: Predicts surrounding context words based on a target word.\n",
        "\n",
        "More about CBOW vs Skip-gram as an overview can be read from [page](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ00X0jpXnkV"
      },
      "source": [
        "![](./cbow_vs_skip_gram.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ5w0rFbXnkV"
      },
      "source": [
        "As of this tutorial, we are going to use Skip-gram (because it produces more accurate results on large datasets!) which in contrast to CBOW considers center word as input as depicted in the figure above and predicts the context words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXWoNARKXnkW",
        "outputId": "5ccdea77-aa58-413a-a5bd-82ef86ad52f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/waydant/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/waydant/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "nltk.download('punkt')  # Download for sentence tokenization\n",
        "nltk.download('stopwords')  # Download stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eTzB2w7VY4mi"
      },
      "outputs": [],
      "source": [
        "# Loads subtitles from an SRT file, extracts dialogue, and performs preprocessing.\n",
        "def load_and_preprocess_subtitles(file_path):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        lines = f.readlines()\n",
        "    dialogue_lines = []\n",
        "    current_dialogue = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.isdigit():      # Check if the line is a subtitle index\n",
        "            if current_dialogue:\n",
        "                dialogue_lines.append(current_dialogue)\n",
        "                current_dialogue = []\n",
        "        elif line:\n",
        "            current_dialogue.append(line)\n",
        "    if current_dialogue:\n",
        "        dialogue_lines.append(current_dialogue)\n",
        "    # Preprocessing\n",
        "    processed_text = []                                     # A list of preprocessed sentences (each sentence is a list of tokens).\n",
        "    for dialogue in dialogue_lines:\n",
        "        sentences = nltk.sent_tokenize(' '.join(dialogue))  # Tokenize into sentences\n",
        "        tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "        # Remove stop words and convert to lowercase\n",
        "        processed_sentences = []\n",
        "        for sentence in tokenized_sentences:\n",
        "            words = [word.lower() for word in sentence if word.isalpha() and word not in stop_words]\n",
        "            processed_sentences.append(words)\n",
        "        processed_text.extend(processed_sentences)\n",
        "    return processed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fsIIEXTcd70_"
      },
      "outputs": [],
      "source": [
        "# # Training the Word2Vec Model (Skip-gram)\n",
        "# def train_and_save_model(preprocessed_subtitles):\n",
        "#   model = Word2Vec(sentences=preprocessed_subtitles, vector_size=100, window=5, min_count=5, workers=4)\n",
        "#   model.save(f'{subtitle_file[:-4]}.model')\n",
        "#   return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "NEWsCoM6eh7A"
      },
      "outputs": [],
      "source": [
        "# clustering the model using k_means\n",
        "def clustering(model, num_clusters = 10, seed = 0):\n",
        "  word_vectors = model.wv.vectors\n",
        "  # num_clusters = 10  # Adjust as needed\n",
        "  kmeans = KMeans(n_clusters=num_clusters, random_state = seed, max_iter=5000)\n",
        "  kmeans.fit(word_vectors)\n",
        "  cluster_labels = kmeans.labels_\n",
        "  return cluster_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "owmaWHwhelzO"
      },
      "outputs": [],
      "source": [
        "# printing the top 5 words from each cluster\n",
        "def analyzing_clusters(num_clusters, cluster_labels, model):\n",
        "  clusters = []\n",
        "  for cluster_id in range(num_clusters):\n",
        "      print(f\"Cluster {cluster_id}:\")\n",
        "      words_in_cluster = []\n",
        "      for i, label in enumerate(cluster_labels):\n",
        "          if label == cluster_id:\n",
        "              words_in_cluster.append(model.wv.index_to_key[i])\n",
        "      clusters.append(words_in_cluster)\n",
        "      print(words_in_cluster[:15])\n",
        "  # return clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "aZvnUfHUEk_b"
      },
      "outputs": [],
      "source": [
        "subtitle_file = './Titanic.1997.720p.BluRay.X264-AMIABLE.srt'\n",
        "preprocessed_subtitles = load_and_preprocess_subtitles(subtitle_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['meters'], ['you', 'see'], ['okay', 'take', 'bow', 'rail'], ['mir', 'going', 'bow'], ['stay', 'us']]\n"
          ]
        }
      ],
      "source": [
        "print(preprocessed_subtitles[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__preprocessed_subtitles__ is a list of subtitles, i.e. each element of the list is a subtitle on a given timestamp.\n",
        "Each subtitle is further tokenized into tokens (words), hence __preprocessed_subtitles__ is a list of lists of token "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "qciv1CIvE5t_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 2608\n",
            "Number of words: 7214\n",
            "Number of unique words: 1861\n"
          ]
        }
      ],
      "source": [
        "# Analyzing the preprocessed_subtitles\n",
        "print(f'Number of sentences: {len(preprocessed_subtitles)}')\n",
        "total_words = 0\n",
        "for sentence in preprocessed_subtitles:\n",
        "    total_words += len(sentence)\n",
        "print(f'Number of words: {total_words}')\n",
        "# Unique words\n",
        "unique_words = set()\n",
        "for sentence in preprocessed_subtitles:\n",
        "    unique_words.update(sentence)\n",
        "print(f'Number of unique words: {len(unique_words)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we try to create and train our word 2 vector embeddings, using the [word2vec function](https://radimrehurek.com/gensim/models/word2vec.html) from the gensim library.\n",
        "\n",
        "\n",
        "```\n",
        "def train_and_save_model(preprocessed_subtitles):\n",
        "    model = Word2Vec(sentences=preprocessed_subtitles, vector_size=100, window=5, min_count=5, workers=4)\n",
        "    model.save(f'{subtitle_file[:-4]}.model')\n",
        "    return model\n",
        "```\n",
        "\n",
        "We notice that model training takes some arguments namely:\\\n",
        "`sentences`: Our input (list of lists of token)\\\n",
        "`vector_size`: Dimensionality of the word vectors\\\n",
        "`window`: Maximum distance between the current and predicted word within a sentence\\\n",
        "`min_count`: Ignores all words with total frequency lower than this\\\n",
        "`workers`: Use these many worker threads to train the model (=faster training with multicore machines)\\\n",
        "\n",
        "arguments namely `sentences`, `min_count`, and `workers` are normally fixed. We consider `vector_size` and `window` as our 2 parameters of our model of which `window = 5` seems to be a good choice, as all the sentences of our corpus are a subtitle at a given timestamp, which are not long than around 7-8 words, hence 5 covers almost avg of it.\\\n",
        "The only major parameter left to tune is `vector_size`. If the `vector_size` is very low, then many words will be unnecessary be close due to low degrees of freedom for all words, and if the `vector_size` is very high, then all words will have their own axis (dimension), hence increasing the distance tremendously.\\\n",
        "\n",
        "This demands us to find an optimal value of `vector_size`\n",
        "\n",
        "To do so, I try different values of `vector_size` (100, 80, 70, 50, 20), see their results for some fixed words (`titanic`, `love`, `iceberg`, `rose`, `boat`, `deck`, `please`) which are quiet famous and frequently spoken among the film.\n",
        "\n",
        "We chose that value of `vector_size`, which gives a good similar words for these set of fixed, chosen words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word similar to titanic: [('sir', 0.3945820927619934), ('i', 0.38511890172958374), ('hit', 0.3646306097507477), ('together', 0.32710516452789307), ('half', 0.3099047839641571), ('the', 0.30563515424728394), ('happened', 0.30544808506965637), ('pull', 0.300693541765213), ('ever', 0.2866310179233551), ('hey', 0.2863127589225769)]\n",
            "Word similar to love: [('boat', 0.36438918113708496), ('old', 0.3055638372898102), ('i', 0.2953718602657318), ('sir', 0.28493836522102356), ('happened', 0.2845050096511841), ('hear', 0.28200140595436096), ('na', 0.2813623547554016), ('aboard', 0.27632150053977966), ('work', 0.27279940247535706), ('well', 0.26675277948379517)]\n",
            "Word similar to iceberg: [('come', 0.3081974387168884), ('captain', 0.25188523530960083), ('my', 0.2509388029575348), ('knows', 0.23563098907470703), ('why', 0.22201500833034515), ('cal', 0.20967771112918854), ('done', 0.20612716674804688), ('darling', 0.20442481338977814), ('better', 0.20145520567893982), ('na', 0.20035365223884583)]\n",
            "Word similar to rose: [('i', 0.44370338320732117), ('sir', 0.3764584958553314), ('miss', 0.35617193579673767), ('is', 0.3553028404712677), ('and', 0.35225290060043335), ('boats', 0.35204118490219116), ('deck', 0.3477214574813843), ('hold', 0.3468542993068695), ('this', 0.33899885416030884), ('can', 0.3259972929954529)]\n",
            "Word similar to boat: [('love', 0.36438921093940735), ('hands', 0.3548239767551422), ('wanted', 0.3529486656188965), ('i', 0.2786557674407959), ('aboard', 0.25974681973457336), ('tell', 0.25479328632354736), ('a', 0.25157079100608826), ('he', 0.2508223056793213), ('darling', 0.2492440640926361), ('sir', 0.24676957726478577)]\n",
            "Word similar to deck: [('rose', 0.34772151708602905), ('and', 0.3148902356624603), ('will', 0.3147323727607727), ('i', 0.31440040469169617), ('quite', 0.3135584890842438), ('aboard', 0.30196335911750793), ('something', 0.2979066073894501), ('know', 0.28561317920684814), ('new', 0.265607625246048), ('way', 0.2529412508010864)]\n",
            "Word similar to jack: [('back', 0.3740282654762268), ('ship', 0.3401173949241638), ('lovett', 0.3383563756942749), ('just', 0.3356705904006958), ('right', 0.33459243178367615), ('anyone', 0.33230265974998474), ('years', 0.32406118512153625), ('and', 0.31703799962997437), ('i', 0.3161979913711548), ('us', 0.3108592629432678)]\n"
          ]
        }
      ],
      "source": [
        "def train_and_save_model(preprocessed_subtitles):\n",
        "    model = Word2Vec(sentences=preprocessed_subtitles, vector_size=100, window=5, min_count=5, workers=4)\n",
        "    model.save(f'{subtitle_file[:-4]}.model')\n",
        "    return model\n",
        "\n",
        "model = train_and_save_model(preprocessed_subtitles)\n",
        "\n",
        "print(f'Word similar to titanic: {model.wv.most_similar(\"titanic\")}')\n",
        "print(f'Word similar to love: {model.wv.most_similar(\"love\")}')\n",
        "print(f'Word similar to iceberg: {model.wv.most_similar(\"iceberg\")}')\n",
        "print(f'Word similar to rose: {model.wv.most_similar(\"rose\")}')\n",
        "print(f'Word similar to boat: {model.wv.most_similar(\"boat\")}')\n",
        "print(f'Word similar to deck: {model.wv.most_similar(\"deck\")}')\n",
        "print(f'Word similar to jack: {model.wv.most_similar(\"please\")}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word similar to titanic: [('anyone', 0.4070855379104614), ('dawson', 0.31828442215919495), ('us', 0.3125494420528412), ('something', 0.3031032681465149), ('and', 0.27789169549942017), ('iceberg', 0.2750662565231323), ('america', 0.26844125986099243), ('class', 0.26400020718574524), ('so', 0.2630924582481384), ('thing', 0.26143479347229004)]\n",
            "Word similar to love: [('go', 0.3393397331237793), ('tell', 0.3255135715007782), ('never', 0.31151071190834045), ('brown', 0.2856380045413971), ('keep', 0.27667728066444397), ('brandy', 0.2730826437473297), ('she', 0.2656187117099762), ('rose', 0.2624938189983368), ('they', 0.2559216618537903), ('calvert', 0.25108522176742554)]\n",
            "Word similar to iceberg: [('i', 0.4458892047405243), ('dawson', 0.4402019679546356), ('your', 0.4106682240962982), ('the', 0.4049699604511261), ('good', 0.3983312249183655), ('course', 0.37217411398887634), ('ever', 0.3696453869342804), ('please', 0.3511834144592285), ('ship', 0.34790533781051636), ('help', 0.33476921916007996)]\n",
            "Word similar to rose: [('bloody', 0.43421873450279236), ('room', 0.37945735454559326), ('and', 0.37311577796936035), ('sink', 0.3698669970035553), ('hand', 0.3626948595046997), ('anyone', 0.3497500717639923), ('thing', 0.3445870578289032), ('ever', 0.3347821831703186), ('tell', 0.32584527134895325), ('saw', 0.3237592279911041)]\n",
            "Word similar to boat: [('let', 0.3610984981060028), ('i', 0.3311556279659271), ('children', 0.3159988224506378), ('couple', 0.315875768661499), ('iceberg', 0.3092159032821655), ('cut', 0.3024100959300995), ('she', 0.30236825346946716), ('girl', 0.2856289744377136), ('always', 0.2855628728866577), ('you', 0.2813539505004883)]\n",
            "Word similar to deck: [('happened', 0.38133370876312256), ('can', 0.34774044156074524), ('please', 0.3164757490158081), ('we', 0.3125593960285187), ('gon', 0.293342649936676), ('watch', 0.2886142134666443), ('water', 0.2794654071331024), ('go', 0.2790081799030304), ('hey', 0.2779342830181122), ('nice', 0.2699401080608368)]\n",
            "Word similar to jack: [('i', 0.47120949625968933), ('think', 0.4253195524215698), ('go', 0.397165447473526), ('now', 0.3689259886741638), ('going', 0.36250579357147217), ('they', 0.3523014485836029), ('iceberg', 0.3511834442615509), ('it', 0.34602805972099304), ('ever', 0.3459742069244385), ('man', 0.3392220735549927)]\n"
          ]
        }
      ],
      "source": [
        "def train_and_save_model(preprocessed_subtitles):\n",
        "    model = Word2Vec(sentences=preprocessed_subtitles, vector_size=80, window=5, min_count=5, workers=4)\n",
        "    model.save(f'{subtitle_file[:-4]}.model')\n",
        "    return model\n",
        "\n",
        "model = train_and_save_model(preprocessed_subtitles)\n",
        "\n",
        "# num_clusters = 5\n",
        "# cluster_labels = clustering(model, num_clusters)\n",
        "\n",
        "print(f'Word similar to titanic: {model.wv.most_similar(\"titanic\")}')\n",
        "print(f'Word similar to love: {model.wv.most_similar(\"love\")}')\n",
        "print(f'Word similar to iceberg: {model.wv.most_similar(\"iceberg\")}')\n",
        "print(f'Word similar to rose: {model.wv.most_similar(\"rose\")}')\n",
        "print(f'Word similar to boat: {model.wv.most_similar(\"boat\")}')\n",
        "print(f'Word similar to deck: {model.wv.most_similar(\"deck\")}')\n",
        "print(f'Word similar to jack: {model.wv.most_similar(\"please\")}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word similar to titanic: [('ship', 0.49849364161491394), ('guys', 0.41910961270332336), ('you', 0.389367938041687), ('is', 0.3868667483329773), ('find', 0.3857738971710205), ('ice', 0.3838617205619812), ('something', 0.3803415596485138), ('fine', 0.37639692425727844), ('back', 0.3707239031791687), ('quite', 0.3657892942428589)]\n",
            "Word similar to love: [('brown', 0.45342084765434265), ('now', 0.36143332719802856), ('sir', 0.3531278073787689), ('brandy', 0.34103673696517944), ('iceberg', 0.3234598934650421), ('murdoch', 0.31055888533592224), ('son', 0.29757532477378845), ('heart', 0.2944334149360657), ('perhaps', 0.2933398485183716), ('need', 0.2892705798149109)]\n",
            "Word similar to iceberg: [('enough', 0.367948055267334), ('damn', 0.35731348395347595), ('love', 0.3234598934650421), ('long', 0.3168860375881195), ('brown', 0.2827645242214203), ('everything', 0.26591435074806213), ('look', 0.255199134349823), ('couple', 0.24122214317321777), ('man', 0.22336512804031372), ('without', 0.22187113761901855)]\n",
            "Word similar to rose: [('well', 0.46506622433662415), ('ship', 0.4141918122768402), ('is', 0.38898882269859314), ('i', 0.38811975717544556), ('gone', 0.3786468207836151), ('na', 0.3773651421070099), ('side', 0.376924067735672), ('die', 0.36187151074409485), ('main', 0.36153364181518555), ('back', 0.3612403869628906)]\n",
            "Word similar to boat: [('something', 0.38798531889915466), ('right', 0.3790128827095032), ('main', 0.3712277114391327), ('done', 0.3612457513809204), ('last', 0.34779441356658936), ('now', 0.3377823531627655), ('promise', 0.32379841804504395), ('i', 0.32344409823417664), ('call', 0.320972204208374), ('hockley', 0.3206759989261627)]\n",
            "Word similar to deck: [('gon', 0.4011746048927307), ('he', 0.3904460668563843), ('rose', 0.355935275554657), ('hold', 0.34879404306411743), ('minutes', 0.3391311466693878), ('could', 0.33071577548980713), ('ship', 0.3277711570262909), ('and', 0.32320886850357056), ('the', 0.31945666670799255), ('na', 0.31604263186454773)]\n",
            "Word similar to jack: [('ever', 0.3996511399745941), ('call', 0.3981073200702667), ('real', 0.39239904284477234), ('sir', 0.38971665501594543), ('us', 0.3859466314315796), ('ship', 0.3839874565601349), ('but', 0.35772302746772766), ('na', 0.35734742879867554), ('new', 0.3423718512058258), ('last', 0.3283170461654663)]\n"
          ]
        }
      ],
      "source": [
        "def train_and_save_model(preprocessed_subtitles):\n",
        "    model = Word2Vec(sentences=preprocessed_subtitles, vector_size=70, window=5, min_count=5, workers=4)\n",
        "    model.save(f'{subtitle_file[:-4]}.model')\n",
        "    return model\n",
        "\n",
        "model = train_and_save_model(preprocessed_subtitles)\n",
        "\n",
        "# num_clusters = 5\n",
        "# cluster_labels = clustering(model, num_clusters)\n",
        "\n",
        "print(f'Word similar to titanic: {model.wv.most_similar(\"titanic\")}')\n",
        "print(f'Word similar to love: {model.wv.most_similar(\"love\")}')\n",
        "print(f'Word similar to iceberg: {model.wv.most_similar(\"iceberg\")}')\n",
        "print(f'Word similar to rose: {model.wv.most_similar(\"rose\")}')\n",
        "print(f'Word similar to boat: {model.wv.most_similar(\"boat\")}')\n",
        "print(f'Word similar to deck: {model.wv.most_similar(\"deck\")}')\n",
        "print(f'Word similar to jack: {model.wv.most_similar(\"please\")}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word similar to titanic: [('aboard', 0.4349234700202942), ('think', 0.42876994609832764), ('listen', 0.42804190516471863), ('cal', 0.3959748446941376), ('i', 0.37290701270103455), ('got', 0.34954121708869934), ('gon', 0.34775510430336), ('ahead', 0.34508034586906433), ('another', 0.3394218683242798), ('sir', 0.32922300696372986)]\n",
            "Word similar to love: [('everything', 0.33072367310523987), ('quite', 0.31206467747688293), ('long', 0.3052246570587158), ('time', 0.2968745231628418), ('god', 0.282552033662796), ('hard', 0.2791566252708435), ('faster', 0.2746122479438782), ('ship', 0.27238729596138), ('couple', 0.26196300983428955), ('two', 0.26073315739631653)]\n",
            "Word similar to iceberg: [('size', 0.37815576791763306), ('these', 0.3779808282852173), ('ever', 0.37731119990348816), ('us', 0.33595892786979675), ('like', 0.3279488682746887), ('main', 0.29148629307746887), ('inside', 0.28641271591186523), ('eyes', 0.26311543583869934), ('working', 0.26180970668792725), ('ocean', 0.25781452655792236)]\n",
            "Word similar to rose: [('promise', 0.5315784811973572), ('night', 0.44253066182136536), ('i', 0.39771169424057007), ('thing', 0.39015117287635803), ('woman', 0.38710710406303406), ('make', 0.3596782088279724), ('fiancée', 0.3593454360961914), ('women', 0.3452972173690796), ('old', 0.3307082951068878), ('money', 0.32643845677375793)]\n",
            "Word similar to boat: [('shit', 0.4239596426486969), ('anything', 0.3690337538719177), ('watch', 0.32801327109336853), ('boss', 0.3237023651599884), ('ca', 0.3067292273044586), ('sorry', 0.30404821038246155), ('called', 0.29601892828941345), ('starboard', 0.2910463511943817), ('lot', 0.2908501923084259), ('calvert', 0.28678134083747864)]\n",
            "Word similar to deck: [('go', 0.4616129398345947), ('hold', 0.4136790335178375), ('did', 0.35643884539604187), ('you', 0.3536926507949829), ('perhaps', 0.34786295890808105), ('hit', 0.34675416350364685), ('ship', 0.3448338210582733), ('luck', 0.3447399139404297), ('us', 0.33598583936691284), ('course', 0.3328467011451721)]\n",
            "Word similar to jack: [('women', 0.32932114601135254), ('could', 0.32216116786003113), ('life', 0.31762608885765076), ('get', 0.3093116879463196), ('half', 0.29590290784835815), ('fine', 0.276450514793396), ('how', 0.2749113440513611), ('will', 0.26326555013656616), ('go', 0.2525130808353424), ('here', 0.2510852515697479)]\n"
          ]
        }
      ],
      "source": [
        "def train_and_save_model(preprocessed_subtitles):\n",
        "    model = Word2Vec(sentences=preprocessed_subtitles, vector_size=50, window=5, min_count=5, workers=4)\n",
        "    model.save(f'{subtitle_file[:-4]}.model')\n",
        "    return model\n",
        "\n",
        "model = train_and_save_model(preprocessed_subtitles)\n",
        "\n",
        "# num_clusters = 5\n",
        "# cluster_labels = clustering(model, num_clusters)\n",
        "\n",
        "print(f'Word similar to titanic: {model.wv.most_similar(\"titanic\")}')\n",
        "print(f'Word similar to love: {model.wv.most_similar(\"love\")}')\n",
        "print(f'Word similar to iceberg: {model.wv.most_similar(\"iceberg\")}')\n",
        "print(f'Word similar to rose: {model.wv.most_similar(\"rose\")}')\n",
        "print(f'Word similar to boat: {model.wv.most_similar(\"boat\")}')\n",
        "print(f'Word similar to deck: {model.wv.most_similar(\"deck\")}')\n",
        "print(f'Word similar to jack: {model.wv.most_similar(\"please\")}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word similar to titanic: [('room', 0.7159054279327393), ('of', 0.7143035531044006), ('i', 0.6112399697303772), ('gon', 0.5887091159820557), ('keep', 0.5584520697593689), ('the', 0.5373401641845703), ('know', 0.5352386832237244), ('next', 0.5151069760322571), ('left', 0.5084133148193359), ('lads', 0.5022823214530945)]\n",
            "Word similar to love: [('class', 0.6258763670921326), ('that', 0.4601539969444275), ('work', 0.42511507868766785), ('cold', 0.39505261182785034), ('move', 0.38745594024658203), ('my', 0.3805496096611023), ('hell', 0.36937204003334045), ('stay', 0.33506134152412415), ('really', 0.33404409885406494), ('to', 0.31649693846702576)]\n",
            "Word similar to iceberg: [('keep', 0.5477943420410156), ('die', 0.5453478693962097), ('step', 0.5326889157295227), ('room', 0.5202898979187012), ('without', 0.47534671425819397), ('what', 0.45491838455200195), ('time', 0.45263946056365967), ('take', 0.448720246553421), ('hard', 0.40828588604927063), ('boats', 0.3916594386100769)]\n",
            "Word similar to rose: [('the', 0.5751013159751892), ('know', 0.5712829828262329), ('everything', 0.5703719258308411), ('diamond', 0.5695179104804993), ('wind', 0.5636074542999268), ('gentlemen', 0.5501331090927124), ('brock', 0.5367745757102966), ('together', 0.4967288672924042), ('hands', 0.4928015172481537), ('new', 0.49195539951324463)]\n",
            "Word similar to boat: [('bloody', 0.7560256123542786), ('new', 0.6211935877799988), ('well', 0.593257486820221), ('believe', 0.5579783916473389), ('course', 0.5348093509674072), ('make', 0.5228167176246643), ('why', 0.5218003988265991), ('lovett', 0.5201468467712402), ('shut', 0.4949333965778351), ('together', 0.49485060572624207)]\n",
            "Word similar to deck: [('hand', 0.6498786807060242), ('know', 0.6063432693481445), ('make', 0.5873730778694153), ('sir', 0.5838749408721924), ('see', 0.553013026714325), ('new', 0.5518186688423157), ('i', 0.5316938757896423), ('well', 0.5274242758750916), ('stay', 0.5226908922195435), ('night', 0.5111534595489502)]\n",
            "Word similar to jack: [('miss', 0.6701169610023499), ('aboard', 0.6519327163696289), ('think', 0.5857717990875244), ('shit', 0.5642608404159546), ('sir', 0.5388047099113464), ('god', 0.516052782535553), ('heart', 0.5067445635795593), ('best', 0.5059966444969177), ('but', 0.4992876350879669), ('i', 0.49189937114715576)]\n"
          ]
        }
      ],
      "source": [
        "def train_and_save_model(preprocessed_subtitles):\n",
        "    model = Word2Vec(sentences=preprocessed_subtitles, vector_size=20, window=5, min_count=5, workers=4)\n",
        "    model.save(f'{subtitle_file[:-4]}.model')\n",
        "    return model\n",
        "\n",
        "model = train_and_save_model(preprocessed_subtitles)\n",
        "\n",
        "# num_clusters = 5\n",
        "# cluster_labels = clustering(model, num_clusters)\n",
        "\n",
        "print(f'Word similar to titanic: {model.wv.most_similar(\"titanic\")}')\n",
        "print(f'Word similar to love: {model.wv.most_similar(\"love\")}')\n",
        "print(f'Word similar to iceberg: {model.wv.most_similar(\"iceberg\")}')\n",
        "print(f'Word similar to rose: {model.wv.most_similar(\"rose\")}')\n",
        "print(f'Word similar to boat: {model.wv.most_similar(\"boat\")}')\n",
        "print(f'Word similar to deck: {model.wv.most_similar(\"deck\")}')\n",
        "print(f'Word similar to jack: {model.wv.most_similar(\"please\")}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Off all the `vector_size`s, we notice that for `vector_size = 70`, gives decent results\n",
        "\n",
        "```\n",
        "Word similar to titanic: [('ship', 0.49849364161491394), ('guys', 0.41910961270332336), ('you', 0.389367938041687), ('is', 0.3868667483329773), ('find', 0.3857738971710205), ('ice', 0.3838617205619812), ('something', 0.3803415596485138), ('fine', 0.37639692425727844), ('back', 0.3707239031791687), ('quite', 0.3657892942428589)]\n",
        "Word similar to love: [('brown', 0.45342084765434265), ('now', 0.36143332719802856), ('sir', 0.3531278073787689), ('brandy', 0.34103673696517944), ('iceberg', 0.3234598934650421), ('murdoch', 0.31055888533592224), ('son', 0.29757532477378845), ('heart', 0.2944334149360657), ('perhaps', 0.2933398485183716), ('need', 0.2892705798149109)]\n",
        "Word similar to iceberg: [('enough', 0.367948055267334), ('damn', 0.35731348395347595), ('love', 0.3234598934650421), ('long', 0.3168860375881195), ('brown', 0.2827645242214203), ('everything', 0.26591435074806213), ('look', 0.255199134349823), ('couple', 0.24122214317321777), ('man', 0.22336512804031372), ('without', 0.22187113761901855)]\n",
        "Word similar to rose: [('well', 0.46506622433662415), ('ship', 0.4141918122768402), ('is', 0.38898882269859314), ('i', 0.38811975717544556), ('gone', 0.3786468207836151), ('na', 0.3773651421070099), ('side', 0.376924067735672), ('die', 0.36187151074409485), ('main', 0.36153364181518555), ('back', 0.3612403869628906)]\n",
        "Word similar to boat: [('something', 0.38798531889915466), ('right', 0.3790128827095032), ('main', 0.3712277114391327), ('done', 0.3612457513809204), ('last', 0.34779441356658936), ('now', 0.3377823531627655), ('promise', 0.32379841804504395), ('i', 0.32344409823417664), ('call', 0.320972204208374), ('hockley', 0.3206759989261627)]\n",
        "Word similar to deck: [('gon', 0.4011746048927307), ('he', 0.3904460668563843), ('rose', 0.355935275554657), ('hold', 0.34879404306411743), ('minutes', 0.3391311466693878), ('could', 0.33071577548980713), ('ship', 0.3277711570262909), ('and', 0.32320886850357056), ('the', 0.31945666670799255), ('na', 0.31604263186454773)]\n",
        "Word similar to jack: [('ever', 0.3996511399745941), ('call', 0.3981073200702667), ('real', 0.39239904284477234), ('sir', 0.38971665501594543), ('us', 0.3859466314315796), ('ship', 0.3839874565601349), ('but', 0.35772302746772766), ('na', 0.35734742879867554), ('new', 0.3423718512058258), ('last', 0.3283170461654663)]\n",
        "```\n",
        "\n",
        "`titanic` is highly related to `ship` and to `ice`\\\n",
        "`love` is related to `heart`\\\n",
        "`iceberg` is related to `long`, `damn` (expressing a ah! damn! emotion while seeing it)\\\n",
        "`rose` is related to `main` (character), `die` (ending scene)\\\n",
        "`deck` is related to `rose`, `hold`, `ship` (referring to the famous scene on the `ship`, where jack and `rose` `hold` their hands on the `deck`)\\\n",
        "`jack` is related to `sir` (masculine)\n",
        "\n",
        "Among different `vector_size`s (100, 80, 70, 50, 20), these are the most relatable vector-embedding found, hence we finalize:\n",
        "\n",
        "**vector_size = 70**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_save_model(preprocessed_subtitles):\n",
        "    model = Word2Vec(sentences=preprocessed_subtitles, vector_size=70, window=5, min_count=5, workers=4)\n",
        "    model.save(f'{subtitle_file[:-4]}.model')\n",
        "    return model\n",
        "\n",
        "model = train_and_save_model(preprocessed_subtitles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we try to cluster them using k-means (which has a parameter k = number of clusters). We try different values of k, to find the best clusters we can find.\n",
        "\n",
        "We define `best` cluster subjectively, as it is very difficult to have some numerical metrics to evaluate our performance. So we choose such k which clusters `similar` looking words (words which are related to each other, or have similar meanings, or some specific reference in the movie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0:\n",
            "['this', 'hear', 'find', 'first', 'lower', 'another', 'jump', 'worry', 'years', 'thought', 'then', 'sink']\n",
            "Cluster 1:\n",
            "['let', 'good', 'what', 'sorry', 'hello', 'so', 'pull', 'wo', 'nice', 'andrews', 'cal', 'together', 'really', 'woman', 'thing']\n",
            "Cluster 2:\n",
            "['come', 'rose', 'like', 'please', 'way', 'the', 'put', 'yes', 'boats', 'could', 'deck', 'he', 'make', 'move', 'na']\n",
            "Cluster 3:\n",
            "['boat', 'keep', 'look', 'give', 'is', 'but', 'course', 'diamond', 'something', 'just', 'gon', 'hurry', 'men', 'class', 'calvert']\n",
            "Cluster 4:\n",
            "['i', 'it', 'we', 'no', 'sir', 'well', 'now', 'oh', 'got', 'want', 'not', 'side', 'stop', 'anyone', 'best']\n",
            "Cluster 5:\n",
            "['there', 'never', 'hand', 'can', 'nothing', 'of', 'hey', 'brock', 'falls', 'shall', 'gentlemen', 'remember', 'lifebelts', 'if', 'shut']\n",
            "Cluster 6:\n",
            "['right', 'back', 'see', 'one', 'ship', 'hold', 'titanic', 'ca', 'tell', 'left', 'must', 'ahead', 'girl', 'life', 'everything']\n",
            "Cluster 7:\n",
            "['jack', 'and', 'know', 'that', 'okay', 'take', 'dawson', 'think', 'thank', 'stay', 'away', 'turn', 'why', 'here', 'a']\n",
            "Cluster 8:\n",
            "['you', 'go', 'get', 'wait', 'going', 'they', 'god', 'time', 'would', 'need', 'miss', 'man', 'little', 'women', 'anything']\n",
            "Cluster 9:\n",
            "['do', 'us', 'she', 'all', 'help', 'shit', 'fine', 'say', 'full', 'how', 'two', 'son', 'may', 'to', 'said']\n"
          ]
        }
      ],
      "source": [
        "num_clusters = 10\n",
        "cluster_labels = clustering(model, num_clusters)\n",
        "analyzing_clusters(num_clusters, cluster_labels, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0:\n",
            "['back', 'rose', 'no', 'like', 'this', 'well', 'ship', 'now', 'look', 'help', 'titanic', 'but', 'women', 'hear', 'room']\n",
            "Cluster 1:\n",
            "['you', 'come', 'get', 'we', 'see', 'let', 'good', 'one', 'what', 'wait', 'boat', 'oh', 'okay', 'time', 'would']\n",
            "Cluster 2:\n",
            "['put', 'keep', 'yes', 'give', 'ca', 'thank', 'he', 'stay', 'move', 'listen', 'here', 'of', 'shit', 'say', 'two']\n",
            "Cluster 3:\n",
            "['i', 'go', 'right', 'jack', 'and', 'know', 'that', 'do', 'there', 'us', 'please', 'going', 'hold', 'they', 'god']\n",
            "Cluster 4:\n",
            "['it', 'sir', 'way', 'the', 'got', 'she', 'all', 'boats', 'not', 'is', 'turn', 'stop', 'diamond', 'just', 'why']\n"
          ]
        }
      ],
      "source": [
        "num_clusters = 5\n",
        "cluster_labels = clustering(model, num_clusters)\n",
        "analyzing_clusters(num_clusters, cluster_labels, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0:\n",
            "['this', 'all', 'hear', 'find', 'another', 'jump', 'worry', 'thought']\n",
            "Cluster 1:\n",
            "['you', 'let', 'good', 'titanic', 'sorry', 'so', 'here', 'andrews', 'say', 'son', 'woman', 'quite', 'thing', 'sure', 'excuse']\n",
            "Cluster 2:\n",
            "['come', 'rose', 'like', 'please', 'way', 'the', 'put', 'boats', 'could', 'deck', 'make', 'move', 'na', 'anything', 'listen']\n",
            "Cluster 3:\n",
            "['do', 'boat', 'look', 'is', 'but', 'diamond', 'something', 'gon', 'hurry', 'how', 'men', 'class', 'done', 'main', 'working']\n",
            "Cluster 4:\n",
            "['i', 'it', 'we', 'no', 'sir', 'now', 'oh', 'got', 'not', 'side', 'anyone', 'best', 'night', 'new', 'enough']\n",
            "Cluster 5:\n",
            "['there', 'yes', 'god', 'need', 'never', 'can', 'nothing', 'why', 'hey', 'brock', 'shall', 'gentlemen', 'minutes', 'lifebelts', 'if']\n",
            "Cluster 6:\n",
            "['back', 'see', 'one', 'ship', 'hold', 'tell', 'left', 'children', 'girl', 'everything', 'my', 'rail', 'forward', 'happened', 'bloody']\n",
            "Cluster 7:\n",
            "['jack', 'and', 'know', 'okay', 'take', 'dawson', 'thank', 'stay', 'away', 'turn', 'a', 'wo', 'knows', 'money', 'will']\n",
            "Cluster 8:\n",
            "['get', 'wait', 'they', 'time', 'man', 'little', 'women', 'hello', 'course', 'water', 'next', 'may', 'where', 'captain', 'iceberg']\n",
            "Cluster 9:\n",
            "['us', 'she', 'help', 'stop', 'shit', 'fine', 'full', 'two', 'cold', 'to', 'said', 'steady', 'better', 'big', 'called']\n",
            "Cluster 10:\n",
            "['right', 'would', 'give', 'he', 'pull', 'nice', 'cal', 'brown', 'goes', 'remember', 'wanted', 'goddamn', 'for', 'anybody', 'promise']\n",
            "Cluster 11:\n",
            "['that', 'keep', 'ca', 'hand', 'love', 'of', 'together', 'life', 'falls', 'lower', 'even', 'try', 'years', 'heart', 'did']\n",
            "Cluster 12:\n",
            "['go', 'well', 'what', 'going', 'want', 'miss', 'just', 'mother', 'last', 'understand', 'first', 'saw', 'cut', 'old', 'every']\n",
            "Cluster 13:\n",
            "['trust', 'long', 'calvert', 'found']\n",
            "Cluster 14:\n",
            "['think', 'room', 'must', 'ahead', 'hard', 'really', 'ocean', 'eyes', 'actually', 'brandy', 'kind']\n"
          ]
        }
      ],
      "source": [
        "num_clusters = 15\n",
        "cluster_labels = clustering(model, num_clusters)\n",
        "analyzing_clusters(num_clusters, cluster_labels, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0:\n",
            "['all', 'listen', 'find', 'another', 'jump', 'worry', 'thought']\n",
            "Cluster 1:\n",
            "['you', 'let', 'good', 'titanic', 'so', 'here', 'andrews', 'say', 'woman', 'thing', 'sure', 'aboard', 'watch', 'around', 'inside']\n",
            "Cluster 2:\n",
            "['come', 'rose', 'like', 'please', 'yes', 'deck', 'make', 'move', 'people', 'along', 'call', 'check', 'things', 'name', 'seen']\n",
            "Cluster 3:\n",
            "['do', 'boat', 'is', 'diamond', 'gon', 'hurry', 'how', 'class', 'done', 'main', 'working', 'hell']\n",
            "Cluster 4:\n",
            "['i', 'it', 'no', 'sir', 'now', 'oh', 'got', 'not', 'side', 'anyone', 'best', 'night', 'new', 'have', 'believe']\n",
            "Cluster 5:\n",
            "['there', 'god', 'never', 'nothing', 'hey', 'brock', 'shall', 'minutes', 'lifebelts', 'called', 'if', 'shut', 'boss', 'bow', 'faster']\n",
            "Cluster 6:\n",
            "['see', 'one', 'hold', 'tell', 'girl', 'everything', 'my', 'rail', 'forward', 'happened', 'lot']\n",
            "Cluster 7:\n",
            "['jack', 'and', 'know', 'okay', 'take', 'dawson', 'thank', 'stay', 'away', 'turn', 'a', 'wo', 'money', 'first', 'will']\n",
            "Cluster 8:\n",
            "['get', 'wait', 'they', 'time', 'need', 'man', 'little', 'women', 'hello', 'water', 'knows', 'next', 'may', 'where', 'captain']\n",
            "Cluster 9:\n",
            "['us', 'she', 'stop', 'shit', 'fine', 'full', 'two', 'cold', 'to', 'said', 'steady', 'better', 'big', 'couple', 'hit']\n",
            "Cluster 10:\n",
            "['right', 'would', 'he', 'pull', 'nice', 'brown', 'goes', 'remember', 'wanted', 'goddamn', 'for', 'promise']\n",
            "Cluster 11:\n",
            "['that', 'keep', 'ca', 'love', 'of', 'son', 'life', 'lower', 'even', 'try', 'years', 'sink', 'america', 'without']\n",
            "Cluster 12:\n",
            "['go', 'what', 'going', 'want', 'miss', 'just', 'mother', 'last', 'understand', 'saw', 'cut', 'old', 'every', 'hands', 'already']\n",
            "Cluster 13:\n",
            "['trust', 'long', 'calvert', 'found']\n",
            "Cluster 14:\n",
            "['think', 'room', 'something', 'must', 'ahead', 'hard', 'really', 'ocean', 'actually', 'brandy', 'kind']\n",
            "Cluster 15:\n",
            "['this', 'well', 'ship', 'way', 'put', 'look', 'give', 'hockley', 'still', 'enough', 'men', 'iceberg', 'perhaps', 'bloody', 'baby']\n",
            "Cluster 16:\n",
            "['back', 'we', 'the', 'boats', 'but', 'could', 'na', 'hear', 'can', 'course', 'anything', 'children', 'ever', 'went']\n",
            "Cluster 17:\n",
            "['hand', 'why', 'together', 'falls', 'heart', 'dear', 'did', 'guys', 'luck']\n",
            "Cluster 18:\n",
            "['help', 'sorry', 'cal', 'yeah', 'lady', 'anybody']\n",
            "Cluster 19:\n",
            "['left', 'quite', 'excuse', 'world', 'gentlemen', 'half', 'an', 'quickly', 'welcome', 'yet']\n"
          ]
        }
      ],
      "source": [
        "num_clusters = 20\n",
        "cluster_labels = clustering(model, num_clusters)\n",
        "analyzing_clusters(num_clusters, cluster_labels, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Off all tried clusters (k = 5, 10, 15, 20), we observe that the best decent results are found for k = 10.\n",
        "\n",
        "\n",
        "```\n",
        "Cluster 0:\n",
        "['this', 'hear', 'find', 'first', 'lower', 'another', 'jump', 'worry', 'years', 'thought', 'then', 'sink']\n",
        "Cluster 1:\n",
        "['let', 'good', 'what', 'sorry', 'hello', 'so', 'pull', 'wo', 'nice', 'andrews', 'cal', 'together', 'really', 'woman', 'thing']\n",
        "Cluster 2:\n",
        "['come', 'rose', 'like', 'please', 'way', 'the', 'put', 'yes', 'boats', 'could', 'deck', 'he', 'make', 'move', 'na']\n",
        "Cluster 3:\n",
        "['boat', 'keep', 'look', 'give', 'is', 'but', 'course', 'diamond', 'something', 'just', 'gon', 'hurry', 'men', 'class', 'calvert']\n",
        "Cluster 4:\n",
        "['i', 'it', 'we', 'no', 'sir', 'well', 'now', 'oh', 'got', 'want', 'not', 'side', 'stop', 'anyone', 'best']\n",
        "Cluster 5:\n",
        "['there', 'never', 'hand', 'can', 'nothing', 'of', 'hey', 'brock', 'falls', 'shall', 'gentlemen', 'remember', 'lifebelts', 'if', 'shut']\n",
        "Cluster 6:\n",
        "['right', 'back', 'see', 'one', 'ship', 'hold', 'titanic', 'ca', 'tell', 'left', 'must', 'ahead', 'girl', 'life', 'everything']\n",
        "Cluster 7:\n",
        "['jack', 'and', 'know', 'that', 'okay', 'take', 'dawson', 'think', 'thank', 'stay', 'away', 'turn', 'why', 'here', 'a']\n",
        "Cluster 8:\n",
        "['you', 'go', 'get', 'wait', 'going', 'they', 'god', 'time', 'would', 'need', 'miss', 'man', 'little', 'women', 'anything']\n",
        "Cluster 9:\n",
        "['do', 'us', 'she', 'all', 'help', 'shit', 'fine', 'say', 'full', 'how', 'two', 'son', 'may', 'to', 'said']\n",
        "```\n",
        "\n",
        "Justification:\n",
        "\n",
        "- **<ins>Cluster 0</ins>**: {(`sink`, `jump`, `find`) -> a mix of actions mostly related to the sinking of ship, where the people jumped out}\n",
        "- **<ins>Cluster 1</ins>**: {(`hello`, `sorry`) -> polite words, (`let`, `pull`) -> direct requests}\n",
        "- **<ins>Cluster 2</ins>**: {(`come`, `please`, `rose`) -> focus on `rose`'s circle, (`boats`, `deck`) -> related to boat/ship }\n",
        "- **<ins>Cluster 3</ins>**: {(`men`, `class`, `diamond`) -> class division, ~ though not much clear}\n",
        "- **<ins>Cluster 4</ins>**: {(`i`, `we`, `no`, `sir`) -> first person focus, (`now`, `got`, `stop`, `not`) -> sense of abruptness}\n",
        "- **<ins>Cluster 5</ins>**: {(`falls`, `lifebelts`) -> describing the crisis scene of Titanic}\n",
        "- **<ins>Cluster 6</ins>**: {(`titanic`, `ship`) -> related to the ship}\n",
        "- **<ins>Cluster 7</ins>**: {(`jack`, `dawson`) -> Jack Dawson, the hero, (`stay`, `away`) -> opposite meaning, but both showing a sense of nearness}\n",
        "- **<ins>Cluster 8</ins>**: {(`man`, `women`) -> clubbing genders together, (`go`, `going`, `get`) ->  , (`god`, `miss`, `anything`, `need`) -> emotional words; needs being expressed}\n",
        "- **<ins>Cluster 9</ins>**: {(`shit`, `help`, `fine`) -> exclamations, (`may`, `how`) -> uncertainity, (`do`, `help`) -> direct imperatives}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clustering for `vector_size = 70`, and `k = 10` and a different seed gives the following clusters: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0:\n",
            "['know', 'titanic', 'thank', 'here', 'son', 'quite', 'lower', 'saw', 'steady', 'excuse', 'long', 'world', 'called', 'half', 'an']\n",
            "Cluster 1:\n",
            "['you', 'good', 'please', 'ship', 'wait', 'they', 'time', 'all', 'miss', 'make', 'women', 'hello', 'mother', 'water', 'next']\n",
            "Cluster 2:\n",
            "['i', 'go', 'rose', 'jack', 'and', 'no', 'that', 'do', 'well', 'god', 'take', 'want', 'need', 'dawson', 'could']\n",
            "Cluster 3:\n",
            "['like', 'sir', 'boats', 'think', 'is', 'room', 'diamond', 'something', 'wo', 'nice', 'ahead', 'full', 'girl', 'work', 'another']\n",
            "Cluster 4:\n",
            "['we', 'boat', 'would', 'na', 'love', 'andrews', 'best', 'last', 'brown', 'aboard', 'captain', 'door', 'main', 'for', 'baby']\n",
            "Cluster 5:\n",
            "['right', 'see', 'way', 'the', 'hold', 'she', 'not', 'hand', 'stay', 'side', 'tell', 'stop', 'left', 'nothing', 'gon']\n",
            "Cluster 6:\n",
            "['come', 'get', 'this', 'us', 'what', 'going', 'look', 'give', 'help', 'sorry', 'deck', 'find', 'hurry', 'cal', 'together']\n",
            "Cluster 7:\n",
            "['let', 'one', 'there', 'oh', 'never', 'man', 'can', 'turn', 'so', 'course', 'shit', 'hockley', 'cold', 'really', 'falls']\n",
            "Cluster 8:\n",
            "['put', 'keep', 'yes', 'okay', 'ca', 'little', 'he', 'move', 'anything', 'listen', 'pull', 'a', 'people', 'say', 'how']\n",
            "Cluster 9:\n",
            "['it', 'back', 'now', 'got', 'but', 'just', 'anyone', 'fine', 'night', 'shall', 'hands', 'lifebelts', 'check', 'leave', 'done']\n"
          ]
        }
      ],
      "source": [
        "num_clusters = 10\n",
        "cluster_labels = clustering(model, num_clusters, seed=23)\n",
        "analyzing_clusters(num_clusters, cluster_labels, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Cluster 0:\n",
        "['know', 'titanic', 'thank', 'here', 'son', 'quite', 'lower', 'saw', 'steady', 'excuse', 'long', 'world', 'called', 'half', 'an']\n",
        "Cluster 1:\n",
        "['you', 'good', 'please', 'ship', 'wait', 'they', 'time', 'all', 'miss', 'make', 'women', 'hello', 'mother', 'water', 'next']\n",
        "Cluster 2:\n",
        "['i', 'go', 'rose', 'jack', 'and', 'no', 'that', 'do', 'well', 'god', 'take', 'want', 'need', 'dawson', 'could']\n",
        "Cluster 3:\n",
        "['like', 'sir', 'boats', 'think', 'is', 'room', 'diamond', 'something', 'wo', 'nice', 'ahead', 'full', 'girl', 'work', 'another']\n",
        "Cluster 4:\n",
        "['we', 'boat', 'would', 'na', 'love', 'andrews', 'best', 'last', 'brown', 'aboard', 'captain', 'door', 'main', 'for', 'baby']\n",
        "Cluster 5:\n",
        "['right', 'see', 'way', 'the', 'hold', 'she', 'not', 'hand', 'stay', 'side', 'tell', 'stop', 'left', 'nothing', 'gon']\n",
        "Cluster 6:\n",
        "['come', 'get', 'this', 'us', 'what', 'going', 'look', 'give', 'help', 'sorry', 'deck', 'find', 'hurry', 'cal', 'together']\n",
        "Cluster 7:\n",
        "['let', 'one', 'there', 'oh', 'never', 'man', 'can', 'turn', 'so', 'course', 'shit', 'hockley', 'cold', 'really', 'falls']\n",
        "Cluster 8:\n",
        "['put', 'keep', 'yes', 'okay', 'ca', 'little', 'he', 'move', 'anything', 'listen', 'pull', 'a', 'people', 'say', 'how']\n",
        "Cluster 9:\n",
        "['it', 'back', 'now', 'got', 'but', 'just', 'anyone', 'fine', 'night', 'shall', 'hands', 'lifebelts', 'check', 'leave', 'done']\n",
        "```\n",
        "\n",
        "Justification:\n",
        "\n",
        "- **<ins>Cluster 0</ins>**: Reflections, Information, Scattered Focus:  A mix of knowledge-related terms (\"know\", \"saw\"), references to the Titanic, and expressions of gratitude (\"thank\") alongside diverse terms (\"son\", \"world\", \"half\"), suggesting a potentially reflective moment but without a clear thematic focus.\n",
        "- **<ins>Cluster 1</ins>**: Social Focus, Uncertainty, and Time: Heavy on pronouns (\"you\", \"they\"), references to time and waiting, polite words (\"please\", \"hello\"), and a focus on women and the physical environment (\"ship\", \"water\"). It hints at social interactions, uncertainty, and potential focus on female characters in a specific setting.\n",
        "- **<ins>Cluster 2</ins>**: Jack and Rose's Sphere, Immediacy:  Strong presence of Jack, Rose, Dawson, and first-person focus (\"I\", \"and\"),  mixed with action verbs (\"go\", \"do\", \"take\") and expressions of needs and wants, portraying a strong sense of immediacy and potentially focus on the central characters' actions.\n",
        "- **<ins>Cluster 3</ins>**: Varied Focus, Authority, and Luxury:  A mix of polite terms (\"sir\", \"like\"), references to boats, thought processes (\"think\", \"something\"), along with hints of luxury (\"diamond\") and authority figures (potential ship personnel?).\n",
        "- **<ins>Cluster 4</ins>**: \"We\",  Boats, Authority, Love:  Focus on collective action (\"we\", \"would\"), continued boat references, terms relating to authority (Andrews, captain),  and a touch of positive emotion (\"love\", \"best\") and concern (\"baby\").\n",
        "- **<ins>Cluster 5</ins>**: Varied Directions, Female Focus, Immediacy:  Action verbs focused on navigation (\"right\", \"see\", \"way\") interspersed with references to a female (\"she\"),  phrases implying requests or a lack of control (\"not\", \"hold\",  \"stay\", \"nothing\")\n",
        "- **<ins>Cluster 6</ins>**: Actions, Requests, and a Sense of Chaos:  Dominated by action verbs (\"come\", \"get\", \"going\") and pleas for help. The presence of \"cal\"  and \"hurry\" suggests a chaotic scene, potentially involving Cal Hockley.\n",
        "- **<ins>Cluster 7</ins>**: Imperatives, Uncertainty, Masculinity, and Conflict:  Direct commands (\"let\", \"turn\"), uncertainty (\"never\", \"can\", \"so\"), references suggestive of a male figure (\"man\"), and hints of conflict (\"shit\", \"hockley\", \"falls\").\n",
        "- **<ins>Cluster 8</ins>**: Instructions, Uncertainty, Objects:  Focus on actions (\"put\", \"keep\", \"move\")  alongside terms conveying uncertainty (\"yes\", \"okay\", \"how\") and references to objects and people (\"he\", \"a\", \"people\").\n",
        "- **<ins>Cluster 9</ins>**: References to the Present, Action, Control: Terms like \"it\", \"now\", \"got\" and a focus on actions in the present tense (\"back\", \"check\", \"leave\").  The presence of  \"lifebelts\" and \"shall\"  suggests a focus on immediate survival and procedural instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion:\n",
        "- We first tried to understand what are word 2 vector embeddings.\n",
        "- We saw what are the 2 ways for word2vec embedding:\n",
        "    - **CBOW** and **Skip-Gram**\n",
        "* CBOW (Continuous Bag of Words): Predicts a target word based on the surrounding context.\n",
        "* Skip-gram: Predicts surrounding context words based on a target word.\n",
        "- We read a subtitles file and preprocessed it, which converted our corpus to a list of lists of token.\n",
        "\n",
        "- Then we tried to apply the Word2Vec module from the `gensim` library.\n",
        "- But we needed to first estimate `vector_size`, which we found out to be 70 (it turns out that this is a good value for our case, (1861 unique words))\n",
        "- Then we tried to cluster them using k-means, and found out that k = 10 gives the best results.\n",
        "- We tried to justify the clusters, and found out that not all the clusters, and not the points in the cluster makes sense, but the clusters as a whole, gives a good decent sense of the movie.\n",
        "\n",
        "\n",
        "## Future Work\n",
        "- More analysis could have been possible if the character vs dialogue was present in a subtitles file. It could help us figure out if any character dominates any specific cluster providing **character specific attributes**.\n",
        "\n",
        "- One could also try to plot clusters as the plot progresses, to notice how the clusters change throughout the film's runtime. This could help us to link clusters to specific stages of the movie moments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References:\n",
        "https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
        "\n",
        "https://medium.com/@zafaralibagh6/simple-tutorial-on-word-embedding-and-word2vec-43d477624b6d\n",
        "\n",
        "https://radimrehurek.com/gensim/models/word2vec.html\n",
        "\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
